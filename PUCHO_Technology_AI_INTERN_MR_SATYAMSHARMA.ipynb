{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PUCHO_Technology_AI_INTERN_MR_SATYAMSHARMA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tR2F8XhOu4K2",
        "Pcg8gFWLoiRR",
        "Vywm2KNOIZqH",
        "fk9SU6VAIdGF",
        "y9ubC_gz5pY6",
        "vUKhxXG9RCRB",
        "u999u3Rd8Xtr",
        "RsjpjAuKpJdY",
        "My2BZYWfItiW"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-HIFeifr2Cv",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://media.licdn.com/dms/image/C4E0BAQHXlPFZhl6kxQ/company-logo_200_200/0?e=2159024400&v=beta&t=S3U_G37BA8ldc_4ur5d0C-j7NHdEeCq-YLOqabikA5E)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR2F8XhOu4K2",
        "colab_type": "text"
      },
      "source": [
        "# Importing Kaggle Dataset and Pre-Processing!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcg8gFWLoiRR",
        "colab_type": "text"
      },
      "source": [
        "## configuring kaggle and downloading kaggle dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gmSOXr7wMXn",
        "colab_type": "code",
        "outputId": "2161d7a8-0f51-40ab-ecb8-87a0acd9801c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "from google.colab import files\n",
        "!mkdir ../root/.kaggle/\n",
        "!cp ../content/kaggle.json ../root/.kaggle/\n",
        "!pip uninstall -y kaggle\n",
        "!pip install --upgrade pip\n",
        "!pip install kaggle==1.5.6\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "#!kaggle datasets download -d paramaggarwal/fashion-product-images-dataset\n",
        "!kaggle datasets download -d paramaggarwal/fashion-product-images-small"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘../root/.kaggle/’: File exists\n",
            "Uninstalling kaggle-1.5.6:\n",
            "  Successfully uninstalled kaggle-1.5.6\n",
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Processing /root/.cache/pip/wheels/57/4e/e8/bb28d035162fb8f17f8ca5d42c3230e284c6aa565b42b72674/kaggle-1.5.6-cp36-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2019.11.28)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.6.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.0.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n",
            "Installing collected packages: kaggle\n",
            "Successfully installed kaggle-1.5.6\n",
            "fashion-product-images-small.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jguu7PAqxKyE",
        "colab_type": "code",
        "outputId": "7f4d8697-5088-4fc1-a3b1-9552e53fb537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import os \n",
        "print(os.listdir(\"../content/\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'images', 'sample.jpg', 'fashion-product-images-small.zip', 'sample2.jpg', 'styles.csv', 'img2.jpg', 'img1.jpg', 'weights', 'topwear.jpg', 'myresultingzips.zip', 'kaggle.json', 'myntradataset', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArexXg-8wLEl",
        "colab_type": "code",
        "outputId": "1a73b8f2-53b3-4b46-ca8f-c05d849db76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "!unzip fashion-product-images-small.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  fashion-product-images-small.zip\n",
            "replace images/10000.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRfeJI1sotP8",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the Dataset for Training and Testing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vywm2KNOIZqH",
        "colab_type": "text"
      },
      "source": [
        "## data prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdebuc680Tk6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "fd0a0572-1540-4802-f88c-61f8da06e719"
      },
      "source": [
        "import pandas as pd\n",
        "import os \n",
        "DATASET_PATH = \"/content/myntradataset\"\n",
        "print(os.listdir(DATASET_PATH))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['images', 'styles.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmoESpVj0zEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "332f0228-ec33-40d1-bc9f-177b63b0ff93"
      },
      "source": [
        "df = pd.read_csv(DATASET_PATH + \"/\" + \"styles.csv\", nrows=5000, error_bad_lines=False)\n",
        "df['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>masterCategory</th>\n",
              "      <th>subCategory</th>\n",
              "      <th>articleType</th>\n",
              "      <th>baseColour</th>\n",
              "      <th>season</th>\n",
              "      <th>year</th>\n",
              "      <th>usage</th>\n",
              "      <th>productDisplayName</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13472</td>\n",
              "      <td>Women</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Dress</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>White</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2011</td>\n",
              "      <td>Casual</td>\n",
              "      <td>United Colors of Benetton Women Solid White Dress</td>\n",
              "      <td>13472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>47539</td>\n",
              "      <td>Women</td>\n",
              "      <td>Footwear</td>\n",
              "      <td>Shoes</td>\n",
              "      <td>Flats</td>\n",
              "      <td>White</td>\n",
              "      <td>Winter</td>\n",
              "      <td>2012</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Portia Women White Flats</td>\n",
              "      <td>47539.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26155</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Bottomwear</td>\n",
              "      <td>Trousers</td>\n",
              "      <td>Grey</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012</td>\n",
              "      <td>Formal</td>\n",
              "      <td>John Miller Men Striped Grey Trousers</td>\n",
              "      <td>26155.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53137</td>\n",
              "      <td>Women</td>\n",
              "      <td>Footwear</td>\n",
              "      <td>Shoes</td>\n",
              "      <td>Flats</td>\n",
              "      <td>Brown</td>\n",
              "      <td>Winter</td>\n",
              "      <td>2015</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Catwalk Women Brown Sandals</td>\n",
              "      <td>53137.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57932</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Tshirts</td>\n",
              "      <td>White</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012</td>\n",
              "      <td>Casual</td>\n",
              "      <td>French Connection Men White T-shirt</td>\n",
              "      <td>57932.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33027</td>\n",
              "      <td>Women</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Kurtas</td>\n",
              "      <td>Blue</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012</td>\n",
              "      <td>Ethnic</td>\n",
              "      <td>Mother Earth Women Printed Blue Kurta</td>\n",
              "      <td>33027.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>17684</td>\n",
              "      <td>Women</td>\n",
              "      <td>Footwear</td>\n",
              "      <td>Shoes</td>\n",
              "      <td>Casual Shoes</td>\n",
              "      <td>Grey</td>\n",
              "      <td>Winter</td>\n",
              "      <td>2015</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Catwalk Women Ballerina Grey Casual Shoe</td>\n",
              "      <td>17684.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3991</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Tshirts</td>\n",
              "      <td>White</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2011</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Mr.Men Men's Wave Rider White T-shirt</td>\n",
              "      <td>3991.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>24497</td>\n",
              "      <td>Women</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Kurtas</td>\n",
              "      <td>Black</td>\n",
              "      <td>Fall</td>\n",
              "      <td>2011</td>\n",
              "      <td>Ethnic</td>\n",
              "      <td>Mother Earth Women Black Printed Kurta</td>\n",
              "      <td>24497.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>38457</td>\n",
              "      <td>Women</td>\n",
              "      <td>Accessories</td>\n",
              "      <td>Bags</td>\n",
              "      <td>Handbags</td>\n",
              "      <td>Navy Blue</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Baggit Women Navy Blue Handbag</td>\n",
              "      <td>38457.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id gender  ...                                 productDisplayName      image\n",
              "0  13472  Women  ...  United Colors of Benetton Women Solid White Dress  13472.jpg\n",
              "1  47539  Women  ...                           Portia Women White Flats  47539.jpg\n",
              "2  26155    Men  ...              John Miller Men Striped Grey Trousers  26155.jpg\n",
              "3  53137  Women  ...                        Catwalk Women Brown Sandals  53137.jpg\n",
              "4  57932    Men  ...                French Connection Men White T-shirt  57932.jpg\n",
              "5  33027  Women  ...              Mother Earth Women Printed Blue Kurta  33027.jpg\n",
              "6  17684  Women  ...           Catwalk Women Ballerina Grey Casual Shoe  17684.jpg\n",
              "7   3991    Men  ...              Mr.Men Men's Wave Rider White T-shirt   3991.jpg\n",
              "8  24497  Women  ...             Mother Earth Women Black Printed Kurta  24497.jpg\n",
              "9  38457  Women  ...                     Baggit Women Navy Blue Handbag  38457.jpg\n",
              "\n",
              "[10 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk9SU6VAIdGF",
        "colab_type": "text"
      },
      "source": [
        "## visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK_zv-ab8mkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "49b3ce11-0407-4a60-bfc0-69a9afa41bc0"
      },
      "source": [
        "df[['id','image','subCategory']].values\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[13472, '13472.jpg', 'Dress'],\n",
              "       [47539, '47539.jpg', 'Shoes'],\n",
              "       [26155, '26155.jpg', 'Bottomwear'],\n",
              "       ...,\n",
              "       [8111, '8111.jpg', 'Belts'],\n",
              "       [44379, '44379.jpg', 'Fragrance'],\n",
              "       [25983, '25983.jpg', 'Fragrance']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9ubC_gz5pY6",
        "colab_type": "text"
      },
      "source": [
        "# Data preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNlBv3v37uPO",
        "colab_type": "code",
        "outputId": "4e36600b-becf-473e-d5bd-95a4ba1e56e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np # li\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import cv2\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import sys\n",
        "images = []\n",
        "datalabels = []\n",
        "\n",
        "for image, subclass in df[['image','subCategory']].values:\n",
        "    img = Image.open('/content/images/{}'.format(image) )\n",
        "    datalabels.append(subclass)\n",
        "    images.append(img)\n",
        "\n",
        "def preprocess_image_np(image_path, desired_size=(224,224)):\n",
        "    im = load_img(image_path, target_size=(224, 224))\n",
        "    #im = Image.open(image_path)\n",
        "    image = img_to_array(im)\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    \n",
        "    return image\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "N = df.shape[0]\n",
        "x_train = np.empty((N, 224, 224,3), dtype=np.uint8)\n",
        "\n",
        "for i, image_id in enumerate(tqdm(df['id'])):\n",
        "    x_train[i, :, :, :] = preprocess_image_np(f'/content/images/{image_id}.jpg'\n",
        ")\n",
        "    \n",
        "\n",
        "y_train = pd.get_dummies(df['subCategory']).values\n",
        "\n",
        "print(\"whole splitting\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train1, x_val, y_train1, y_val = train_test_split(\n",
        "    x_train, y_train, \n",
        "    test_size=0.30, \n",
        "    random_state=2019\n",
        ")\n",
        "\n",
        "print(\"training data\")\n",
        "print(x_train1.shape)\n",
        "print(y_train1.shape)\n",
        "\n",
        "print(\"test data\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:04<00:00, 1030.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "whole splitting\n",
            "(5000, 224, 224, 3)\n",
            "(5000, 36)\n",
            "training data\n",
            "(3500, 224, 224, 3)\n",
            "(3500, 36)\n",
            "test data\n",
            "(1500, 224, 224, 3)\n",
            "(1500, 36)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUKhxXG9RCRB",
        "colab_type": "text"
      },
      "source": [
        "# modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04mXEcvC9WEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXLBPLBLCJJ8",
        "colab_type": "code",
        "outputId": "3233f288-9d7c-4b61-86a9-b666ad4a575d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "tf.Session(config = config)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.client.session.Session at 0x7f62c9043400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUHP3CslBer3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1Dkst3J77Yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#config = tf.ConfigProto(device_count = {'GPU': 0})\n",
        "#sess = tf.Session(config=config)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU3LfHpF9GVP",
        "colab_type": "code",
        "outputId": "618c6b48-c88f-4b2e-d5d9-5c214693239b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from time import time\n",
        "from keras.callbacks import TensorBoard\n",
        "from math import ceil\n",
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(36, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        " #first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#model4.summary()\n",
        "\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2)\n",
        "\n",
        "\n",
        "model.save('/content/weights/Vgg19_model.h5')\n",
        "model.save_weights('/content/weights/Vgg19_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "5000/5000 [==============================] - 15s 3ms/step - loss: 1.1788 - acc: 0.8234\n",
            "Epoch 2/2\n",
            "5000/5000 [==============================] - 15s 3ms/step - loss: 0.3284 - acc: 0.9260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u999u3Rd8Xtr",
        "colab_type": "text"
      },
      "source": [
        "# Import the Pretrained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UpD9zjfm_O5",
        "colab_type": "text"
      },
      "source": [
        "## using MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqgw4kvq1Tll",
        "colab_type": "code",
        "outputId": "7231fb9d-d534-4092-be56-08b6c42b08e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5627
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from time import time\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import TensorBoard\n",
        "from math import ceil\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(36, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model1 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        " #first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model1.summary()\n",
        "\n",
        "\n",
        "# from math import ceil # import it in beginnning\n",
        "\n",
        "\n",
        "\n",
        "model1.fit(x_train, y_train, batch_size=32, epochs=2)\n",
        "\n",
        "\n",
        "model1.save('/content/weights/MobileNetV2_model.h5')\n",
        "model1.save_weights('/content/weights/MobileNetV2_model_weights.h5')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 2s 0us/step\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_pad (ZeroPadding2D)       (None, 225, 225, 3)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         Conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
            "                                                                 block_2_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
            "                                                                 block_4_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
            "                                                                 block_5_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
            "                                                                 block_7_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
            "                                                                 block_8_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
            "                                                                 block_9_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
            "                                                                 block_11_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
            "                                                                 block_12_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
            "                                                                 block_14_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
            "                                                                 block_15_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1024)         1311744     global_average_pooling2d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 36)           36900       dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,606,628\n",
            "Trainable params: 1,348,644\n",
            "Non-trainable params: 2,257,984\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "5000/5000 [==============================] - 7s 1ms/step - loss: 0.8780 - acc: 0.7810\n",
            "Epoch 2/2\n",
            "5000/5000 [==============================] - 5s 1ms/step - loss: 0.3926 - acc: 0.8872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPwNNbf0nwrl",
        "colab_type": "text"
      },
      "source": [
        "## using Resnet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSwD5l3l6Yxy",
        "colab_type": "code",
        "outputId": "34a153df-5088-4aa7-be43-749348c9797e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6375
        }
      },
      "source": [
        "\n",
        "\n",
        "from math import ceil\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from math import ceil\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(36, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model2 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        " #first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()\n",
        "\n",
        "\n",
        "model2.fit(x_train, y_train, batch_size=32, epochs=2)\n",
        "\n",
        "model2.save('/content/weights/Resnet50_model.h5')\n",
        "model2.save_weights('/content/weights/Resnet50_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 7s 0us/step\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2a (Conv2D)         (None, 56, 56, 64)   4160        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch1 (Conv2D)          (None, 56, 56, 256)  16640       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 56, 56, 256)  1024        res2a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 56, 56, 256)  0           bn2a_branch2c[0][0]              \n",
            "                                                                 bn2a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 56, 56, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 56, 56, 256)  0           bn2b_branch2c[0][0]              \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 56, 56, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 56, 56, 256)  0           bn2c_branch2c[0][0]              \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 56, 56, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n",
            "                                                                 bn3a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 28, 28, 512)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
            "                                                                 bn4a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 14, 14, 1024) 0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
            "                                                                 bn5a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
            "                                                                 activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 2048)         0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 36)           36900       dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 25,722,788\n",
            "Trainable params: 2,135,076\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "5000/5000 [==============================] - 13s 3ms/step - loss: 0.7708 - acc: 0.8016\n",
            "Epoch 2/2\n",
            "5000/5000 [==============================] - 11s 2ms/step - loss: 0.3358 - acc: 0.8998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlTpUqDP3NEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2UkVTeY2SVt",
        "colab_type": "text"
      },
      "source": [
        "## using VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0kKlsIb6_ko",
        "colab_type": "code",
        "outputId": "058991dd-0f0e-4bb0-e331-11d223b7d377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        }
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from math import ceil\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(36, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model3 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        " #first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.summary()\n",
        "\n",
        "\n",
        "model3.fit(x_train, y_train, batch_size=32, epochs=2)\n",
        "\n",
        "model3.save('/content/weights/Vgg16_model.h5')\n",
        "model3.save_weights('/content/weights/Vgg16_model_weights.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 5s 0us/step\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_5 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 36)                36900     \n",
            "=================================================================\n",
            "Total params: 15,276,900\n",
            "Trainable params: 562,212\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "5000/5000 [==============================] - 14s 3ms/step - loss: 1.1344 - acc: 0.8090\n",
            "Epoch 2/2\n",
            "5000/5000 [==============================] - 13s 3ms/step - loss: 0.2952 - acc: 0.9230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJXOWWUF6_hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fcjFg4lD4XUP"
      },
      "source": [
        "## using VGG19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8yFP1B16_e5",
        "colab_type": "code",
        "outputId": "4600ce7b-c0af-4959-9437-9e953f6ed492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1025
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from time import time\n",
        "from keras.callbacks import TensorBoard\n",
        "from math import ceil\n",
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(36, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model4 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        " #first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model4.summary()\n",
        "\n",
        "\n",
        "model4.fit(x_train, y_train, batch_size=32, epochs=2)\n",
        "model4.save('/content/weights/Vgg19_2_model.h5')\n",
        "model4.save_weights('/content/weights/Vgg19_2_model_weights.h5')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_6 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 36)                36900     \n",
            "=================================================================\n",
            "Total params: 20,586,596\n",
            "Trainable params: 562,212\n",
            "Non-trainable params: 20,024,384\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "5000/5000 [==============================] - 16s 3ms/step - loss: 0.9219 - acc: 0.8214\n",
            "Epoch 2/2\n",
            "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2558 - acc: 0.9322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fk2DW2vCHfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WVM9OBW-5WEE"
      },
      "source": [
        "## using Xception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAFxfB2P6_Vz",
        "colab_type": "code",
        "outputId": "6c8bd8ad-b00c-4132-c402-4a84d731470b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1025
        }
      },
      "source": [
        "from keras.applications.xception import Xception\n",
        "from time import time\n",
        "from keras.callbacks import TensorBoard\n",
        "from math import ceil\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(36, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model5 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        " #first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model5.summary()\n",
        "\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "#history = model5.fit(train_x, train_y,validation_split = 0.1, epochs=50, batch_size=4)\n",
        "\n",
        "model5.fit(x_train, y_train, batch_size=32, epochs=2)\n",
        "\n",
        "\n",
        "model5.save('/content/weights/Xception_model.h5')\n",
        "model5.save_weights('/content/weights/Xception_model_weights.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_7 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 36)                36900     \n",
            "=================================================================\n",
            "Total params: 20,586,596\n",
            "Trainable params: 562,212\n",
            "Non-trainable params: 20,024,384\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "5000/5000 [==============================] - 16s 3ms/step - loss: 0.9887 - acc: 0.8194\n",
            "Epoch 2/2\n",
            "5000/5000 [==============================] - 15s 3ms/step - loss: 0.3221 - acc: 0.9250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8CT9kZo-GwM",
        "colab_type": "text"
      },
      "source": [
        "## Inception V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxizlQ4A-XxG",
        "colab_type": "code",
        "outputId": "5bd6462c-a0bc-4f30-a2c1-186f24e73be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11071
        }
      },
      "source": [
        "\n",
        "from math import ceil\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = InceptionV3(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(36, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model6 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        " #first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model6.summary()\n",
        "\n",
        "\n",
        "model6.fit(x_train, y_train, batch_size=32, epochs=2)\n",
        "\n",
        "model6.save('/content/weights/InceptionV3_model.h5')\n",
        "model6.save_weights('/content/weights/InceptionV3_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 7s 0us/step\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 111, 111, 32) 864         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 111, 111, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 111, 111, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 109, 109, 32) 9216        activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 109, 109, 32) 96          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 109, 109, 32) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 109, 109, 64) 18432       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 109, 109, 64) 192         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 109, 109, 64) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 54, 54, 64)   0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 54, 54, 80)   240         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 54, 54, 80)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 52, 52, 192)  138240      activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 52, 52, 192)  576         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 52, 52, 192)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 25, 25, 64)   192         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 25, 25, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 25, 25, 48)   9216        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 25, 25, 96)   55296       activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 25, 25, 48)   144         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 25, 25, 48)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 25, 25, 192)  0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 25, 25, 64)   76800       activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 25, 25, 96)   82944       activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 25, 25, 32)   6144        average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 25, 25, 64)   192         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 25, 25, 96)   288         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 25, 25, 32)   96          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 25, 25, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 25, 25, 96)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 25, 25, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_55[0][0]              \n",
            "                                                                 activation_57[0][0]              \n",
            "                                                                 activation_60[0][0]              \n",
            "                                                                 activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 25, 25, 64)   192         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 25, 25, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 25, 25, 96)   55296       activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 25, 25, 48)   144         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 25, 25, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 25, 25, 48)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 25, 25, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 25, 25, 64)   76800       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 25, 25, 96)   82944       activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 25, 25, 64)   16384       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 25, 25, 64)   192         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 25, 25, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 25, 25, 96)   288         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 25, 25, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 25, 25, 64)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 25, 25, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 25, 25, 96)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 25, 25, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_62[0][0]              \n",
            "                                                                 activation_64[0][0]              \n",
            "                                                                 activation_67[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 25, 25, 64)   192         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 25, 25, 64)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 25, 25, 96)   55296       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 25, 25, 48)   144         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 25, 25, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 25, 25, 48)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 25, 25, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 25, 25, 64)   76800       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 25, 25, 96)   82944       activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 25, 25, 64)   18432       average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 25, 25, 64)   192         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 25, 25, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 25, 25, 96)   288         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 25, 25, 64)   192         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 25, 25, 64)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 25, 25, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 25, 25, 96)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 25, 25, 64)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_69[0][0]              \n",
            "                                                                 activation_71[0][0]              \n",
            "                                                                 activation_74[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 25, 25, 64)   192         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 25, 25, 64)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 25, 25, 96)   55296       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 25, 25, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 25, 25, 96)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 12, 12, 96)   82944       activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 12, 12, 384)  1152        conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 12, 12, 96)   288         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 12, 12, 384)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 12, 12, 96)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_76[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "                                                                 max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 12, 12, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 12, 12, 128)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 12, 12, 128)  114688      activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 12, 12, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 12, 12, 128)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 12, 12, 128)  114688      activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 12, 12, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 12, 12, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 12, 12, 128)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 12, 12, 128)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 12, 12, 128)  114688      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 12, 12, 128)  114688      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 12, 12, 128)  384         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 12, 12, 128)  384         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 12, 12, 128)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 12, 12, 128)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 12, 12, 192)  172032      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 12, 12, 192)  172032      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 12, 12, 192)  576         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 12, 12, 192)  576         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 12, 12, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 12, 12, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 12, 12, 192)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 12, 12, 192)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 12, 12, 192)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 12, 12, 192)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_80[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "                                                                 activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 12, 12, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 12, 12, 160)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 12, 12, 160)  179200      activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 12, 12, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 12, 12, 160)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 12, 12, 160)  179200      activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 12, 12, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 12, 12, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 12, 12, 160)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 12, 12, 160)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 12, 12, 160)  179200      activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 12, 12, 160)  179200      activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 12, 12, 160)  480         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 12, 12, 160)  480         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 12, 12, 160)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 12, 12, 160)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 12, 12, 192)  215040      activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 12, 12, 192)  215040      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 12, 12, 192)  576         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 12, 12, 192)  576         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 12, 12, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 12, 12, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 12, 12, 192)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 12, 12, 192)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 12, 12, 192)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 12, 12, 192)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_90[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "                                                                 activation_98[0][0]              \n",
            "                                                                 activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 12, 12, 160)  179200      activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 12, 12, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 12, 12, 160)  0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 12, 12, 160)  179200      activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 12, 12, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 12, 12, 160)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 12, 12, 160)  179200      activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 12, 12, 160)  179200      activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 12, 12, 160)  480         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 12, 12, 160)  480         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 12, 12, 160)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 12, 12, 160)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 12, 12, 192)  215040      activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 12, 12, 192)  215040      activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 12, 12, 192)  576         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 12, 12, 192)  576         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 12, 12, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 12, 12, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 12, 12, 192)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 12, 12, 192)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 12, 12, 192)  0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 12, 12, 192)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_100[0][0]             \n",
            "                                                                 activation_103[0][0]             \n",
            "                                                                 activation_108[0][0]             \n",
            "                                                                 activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 12, 12, 192)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 12, 12, 192)  258048      activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 12, 12, 192)  258048      activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 12, 12, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 12, 12, 192)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 12, 12, 192)  258048      activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 12, 12, 192)  258048      activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 12, 12, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 12, 12, 192)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 12, 12, 192)  258048      activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 12, 12, 192)  258048      activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 12, 12, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 12, 12, 192)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_110[0][0]             \n",
            "                                                                 activation_113[0][0]             \n",
            "                                                                 activation_118[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 12, 12, 192)  258048      activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 12, 12, 192)  576         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 12, 12, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 12, 12, 192)  0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 12, 12, 192)  0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 5, 5, 320)    552960      activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 5, 5, 192)    331776      activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 5, 5, 320)    960         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 5, 5, 192)    576         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 5, 5, 320)    0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 5, 5, 192)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_121[0][0]             \n",
            "                                                                 activation_125[0][0]             \n",
            "                                                                 max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 5, 5, 448)    1344        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 5, 5, 448)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 5, 5, 384)    1548288     activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 5, 5, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 5, 5, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 5, 5, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 5, 5, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 5, 5, 384)    442368      activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 5, 5, 384)    442368      activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 5, 5, 384)    442368      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 5, 5, 384)    442368      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 5, 5, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 5, 5, 384)    1152        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 5, 5, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 5, 5, 384)    1152        conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 5, 5, 320)    960         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 5, 5, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 5, 5, 384)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 5, 5, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 5, 5, 384)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 5, 5, 192)    576         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 5, 5, 320)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_128[0][0]             \n",
            "                                                                 activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_132[0][0]             \n",
            "                                                                 activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 5, 5, 192)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_126[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 5, 5, 448)    1344        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 5, 5, 448)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 5, 5, 384)    1548288     activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 5, 5, 384)    442368      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 5, 5, 384)    442368      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 5, 5, 384)    442368      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 5, 5, 384)    442368      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 5, 5, 384)    1152        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 5, 5, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 5, 5, 384)    1152        conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 5, 5, 192)    393216      average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 5, 5, 320)    960         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 5, 5, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 5, 5, 384)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 5, 5, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 5, 5, 384)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 5, 5, 192)    576         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 5, 5, 320)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_137[0][0]             \n",
            "                                                                 activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 5, 5, 768)    0           activation_141[0][0]             \n",
            "                                                                 activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 5, 5, 192)    0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_135[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_8 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1024)         2098176     global_average_pooling2d_8[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 36)           36900       dense_15[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 23,937,860\n",
            "Trainable params: 2,135,076\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "5000/5000 [==============================] - 13s 3ms/step - loss: 0.9664 - acc: 0.7624\n",
            "Epoch 2/2\n",
            "5000/5000 [==============================] - 9s 2ms/step - loss: 0.4840 - acc: 0.8662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsjpjAuKpJdY",
        "colab_type": "text"
      },
      "source": [
        "# Choose the Best Model Among the Above based on Accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl-Kq9-XFB1N",
        "colab_type": "text"
      },
      "source": [
        "Accuracies of The 5 Best Models are as follows:(1st training)\n",
        "* **MobileNetV2 :**  acc: 94%         val: 9%\n",
        "* **ResNet50 :**     acc:   94%       val:  67%\n",
        "* **VGG16:**      acc:     79%     val:  80%\n",
        "* **VGG19:**       acc:      89%    val: 88%\n",
        "* **Xception:**       acc:     78%     val: 79%\n",
        "* **inception V3:**       acc:     85%     val: 82%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI4FhGesfPHO",
        "colab_type": "text"
      },
      "source": [
        "NewAccuracies of The 5 Best Models are as follows:((2nd training)\n",
        "* **MobileNetV2 :**  acc: 94%         val: 9%\n",
        "* **ResNet50 :**     acc:   94%       val:  62%\n",
        "* **VGG16:**      acc:     86%     val:  85%\n",
        "* **VGG19:**       acc:      91%    val: 88%\n",
        "* **Xception:**       acc:     72%     val: 86%\n",
        "* **inception V3:**       acc:     85%      val: 80% "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwJHOnLTtVvT",
        "colab_type": "text"
      },
      "source": [
        "NewAccuracies of The 5 Best Models are as follows:((3rd training)\n",
        "* **MobileNetV2 :**  acc: 94%         val: 9%\n",
        "* **ResNet50 :**     acc:   94%       val:  62%\n",
        "* **VGG16:**      acc:     86%     val:  85%\n",
        "* **VGG19:**       acc:      91%    val: 88%\n",
        "* **Xception:**       acc:     72%     val: 86%\n",
        "* **inception V3:**       acc:     85%      val: 81%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFubiVPXgw1g",
        "colab_type": "text"
      },
      "source": [
        "## **VGG19 performs best** with **Acc:91%** and **VACC:88%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My2BZYWfItiW",
        "colab_type": "text"
      },
      "source": [
        "# save all models and weights in zip file for future use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjbpRJgcI0wH",
        "colab_type": "code",
        "outputId": "d413d761-0b69-4757-e9df-55947ecd6486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "!zip -r /content/myresultingzips.zip /content/weights\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/weights/ (stored 0%)\n",
            "  adding: content/weights/Resnet50_model.h5 (deflated 7%)\n",
            "  adding: content/weights/Vgg19_2_model.h5 (deflated 7%)\n",
            "  adding: content/weights/Xception_model.h5 (deflated 7%)\n",
            "  adding: content/weights/Xception_model_weights.h5 (deflated 7%)\n",
            "  adding: content/weights/Vgg19_model_weights.h5 (deflated 7%)\n",
            "  adding: content/weights/Resnet50_model_weights.h5 (deflated 8%)\n",
            "  adding: content/weights/MobileNetV2_model_weights.h5 (deflated 9%)\n",
            "  adding: content/weights/InceptionV3_model_weights.h5 (deflated 8%)\n",
            "  adding: content/weights/Vgg19_2_model_weights.h5 (deflated 7%)\n",
            "  adding: content/weights/Vgg16_model_weights.h5 (deflated 7%)\n",
            "  adding: content/weights/Vgg16_model.h5 (deflated 7%)\n",
            "  adding: content/weights/Vgg19_model.h5 (deflated 7%)\n",
            "  adding: content/weights/InceptionV3_model.h5 (deflated 8%)\n",
            "  adding: content/weights/MobileNetV2_model.h5 (deflated 8%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPtgtypXYN17",
        "colab_type": "text"
      },
      "source": [
        "# Using the weights of the **Best Model out of 6 models** , lets predict an image and give (label,probability)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svXGrjI3McXM",
        "colab_type": "code",
        "outputId": "33b4c068-941d-431a-b7b1-214159b27614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        }
      },
      "source": [
        "# example of using a pre-trained model as a classifier\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.applications.vgg16 import VGG16\n",
        "import json \n",
        "# load an image from file\n",
        "image = load_img('/content/sample.jpg', target_size=(224, 224))\n",
        "# convert the image pixels to a numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for the model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# prepare the image for the VGG model\n",
        "image = preprocess_input(image)\n",
        "# load the model\n",
        "model = load_model('/content/weights/Vgg19_model.h5')\n",
        "#model = VGG16()\n",
        "# predict the probability across all output classes\n",
        "preds = model.predict(image)\n",
        "\n",
        "\n",
        "top = 5\n",
        "\n",
        "\n",
        "labelresults = []\n",
        "for pred in preds:\n",
        "        top_indices = pred.argsort()[-top:][::-1]\n",
        "        result = [datalabels[i] + str(pred[i],) for i in top_indices]\n",
        "        #result.sort(key=lambda x: x[2], reverse=True)\n",
        "        labelresults.append(result)\n",
        "        \n",
        "# convert the probabilities to class labels\n",
        "label = labelresults\n",
        "# retrieve the most likely result, e.g. highest probability\n",
        "label = label[0][0]\n",
        "print(label)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Bags0.86231625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SEO3__I88g6",
        "colab_type": "text"
      },
      "source": [
        "# predicting labels for test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N29OJ6Ow9D5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dac57628-6b79-4e13-94d6-9986911fc157"
      },
      "source": [
        "preds = model.predict(x_val)\n",
        "\n",
        "top = 5  # 5 highest probable labels\n",
        "\n",
        "alllabelresults = []\n",
        "allpredictedlabels = []\n",
        "for pred in preds:\n",
        "        top_indices = pred.argsort()[-top:][::-1]\n",
        "        result = [datalabels[i] + str(pred[i],) for i in top_indices]\n",
        "        #result.sort(key=lambda x: x[2], reverse=True)\n",
        "        alllabelresults.append(result)\n",
        "        \n",
        "\n",
        "# convert the probabilities to class labels and  retrieve the most likely result, e.g. highest probability\n",
        "for label in alllabelresults:\n",
        "    allpredictedlabels.append(label[0])\n",
        "    print(label[0])\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Watches0.9943059\n",
            "Bottomwear0.9997248\n",
            "Bags0.99986434\n",
            "Watches0.9998704\n",
            "Bottomwear0.9924642\n",
            "Bags0.9986872\n",
            "Shoes0.9973127\n",
            "Bags0.9999995\n",
            "Bags0.9992861\n",
            "Topwear0.9935401\n",
            "Bottomwear0.99876827\n",
            "Bags0.88444203\n",
            "Bags0.99889237\n",
            "Bags0.99733424\n",
            "Bags0.9722578\n",
            "Watches0.99999666\n",
            "Bags0.99999976\n",
            "Bags0.9999943\n",
            "Bottomwear0.998262\n",
            "Watches0.9988545\n",
            "Bags0.99994874\n",
            "Bags0.84658843\n",
            "Topwear0.96410894\n",
            "Bags0.98550713\n",
            "Bags0.99871254\n",
            "Bags0.9999939\n",
            "Bottomwear0.9827907\n",
            "Shoes0.8654757\n",
            "Watches0.99994385\n",
            "Shoes0.396372\n",
            "Watches0.99979\n",
            "Watches1.0\n",
            "Wallets0.99994826\n",
            "Bottomwear0.9999939\n",
            "Wallets0.99617696\n",
            "Shoes0.99876875\n",
            "Bottomwear0.99943215\n",
            "Bags0.9937988\n",
            "Shoes0.8736601\n",
            "Topwear0.6036459\n",
            "Bags0.99996984\n",
            "Bags0.99977857\n",
            "Bags0.8554654\n",
            "Shoes0.9500321\n",
            "Bottomwear0.99995244\n",
            "Bags0.9994357\n",
            "Bags0.9999856\n",
            "Bags0.98282987\n",
            "Topwear0.9987834\n",
            "Bags0.9829675\n",
            "Bags0.9971215\n",
            "Bottomwear0.99990356\n",
            "Bottomwear0.99978393\n",
            "Bottomwear0.6527241\n",
            "Bags0.9993962\n",
            "Bags0.99985576\n",
            "Topwear0.9999943\n",
            "Bags0.9998988\n",
            "Bottomwear0.99950826\n",
            "Bottomwear0.6848196\n",
            "Bottomwear0.9995523\n",
            "Bottomwear0.9999753\n",
            "Watches1.0\n",
            "Bottomwear0.99999905\n",
            "Bottomwear0.99983525\n",
            "Bottomwear0.92204404\n",
            "Bags0.9927141\n",
            "Watches0.58712256\n",
            "Watches1.0\n",
            "Bags0.9999945\n",
            "Topwear0.97112966\n",
            "Watches0.9198207\n",
            "Bags0.99999964\n",
            "Shoes0.9983302\n",
            "Bags0.99985576\n",
            "Bottomwear0.99984515\n",
            "Bottomwear0.54244757\n",
            "Wallets0.90972835\n",
            "Bags0.9989812\n",
            "Bottomwear0.9894912\n",
            "Wallets0.9353236\n",
            "Wallets0.85426027\n",
            "Bags0.965858\n",
            "Shoes0.97480124\n",
            "Bags0.98120224\n",
            "Bottomwear0.9997497\n",
            "Topwear0.7420958\n",
            "Bags0.9853807\n",
            "Watches0.9997757\n",
            "Watches0.9994973\n",
            "Shoes0.9999993\n",
            "Bags0.9999825\n",
            "Bottomwear0.9413323\n",
            "Bottomwear0.99968874\n",
            "Bottomwear0.9995365\n",
            "Bags0.99982846\n",
            "Bags0.99999714\n",
            "Bottomwear0.99685127\n",
            "Bottomwear0.9999999\n",
            "Shoes0.9997023\n",
            "Bags0.9998857\n",
            "Watches0.99998295\n",
            "Watches0.99998784\n",
            "Bottomwear1.0\n",
            "Bags0.998257\n",
            "Bags0.8054369\n",
            "Shoes0.75184166\n",
            "Bags0.678826\n",
            "Bottomwear0.99725\n",
            "Bottomwear0.93621093\n",
            "Bags1.0\n",
            "Bottomwear0.9990779\n",
            "Shoes0.9814641\n",
            "Bags0.9999627\n",
            "Topwear0.9785429\n",
            "Shoes0.99979967\n",
            "Bags0.9990232\n",
            "Bags0.99977547\n",
            "Bags0.9999995\n",
            "Watches0.9926035\n",
            "Bottomwear0.99980587\n",
            "Bags0.99988925\n",
            "Bottomwear0.99999654\n",
            "Watches0.96114296\n",
            "Shoes0.9973603\n",
            "Bags0.9938066\n",
            "Watches1.0\n",
            "Bags0.99878055\n",
            "Bottomwear0.99874336\n",
            "Bags0.99987566\n",
            "Bags0.9999716\n",
            "Wallets0.79378814\n",
            "Shoes0.9864677\n",
            "Bags0.99965227\n",
            "Shoes0.83948225\n",
            "Topwear0.98535997\n",
            "Bottomwear0.99561656\n",
            "Bags0.99885833\n",
            "Bottomwear0.9997197\n",
            "Bags0.49803266\n",
            "Wallets0.5148598\n",
            "Bottomwear0.9999999\n",
            "Bottomwear0.99334586\n",
            "Bags0.9999474\n",
            "Topwear0.9738323\n",
            "Bags0.9966428\n",
            "Bags0.99995995\n",
            "Bottomwear0.99970835\n",
            "Bottomwear1.0\n",
            "Bags0.999995\n",
            "Bags0.999997\n",
            "Topwear0.5432429\n",
            "Bags0.999936\n",
            "Bags0.97051144\n",
            "Bags0.55123764\n",
            "Watches1.0\n",
            "Shoes0.99285054\n",
            "Shoes0.99985087\n",
            "Bags0.99797374\n",
            "Bottomwear0.999808\n",
            "Bottomwear0.99993324\n",
            "Topwear0.89605576\n",
            "Shoes0.6457135\n",
            "Bags0.9924826\n",
            "Bags0.9986355\n",
            "Shoes0.8711157\n",
            "Bottomwear0.99999833\n",
            "Topwear0.95491225\n",
            "Bags0.49256185\n",
            "Topwear0.5240678\n",
            "Watches0.9537872\n",
            "Bottomwear0.99672556\n",
            "Shoes0.99313813\n",
            "Bags0.9995845\n",
            "Shoes0.22375315\n",
            "Topwear0.99983466\n",
            "Bags0.99994063\n",
            "Watches1.0\n",
            "Bottomwear0.9944851\n",
            "Shoes0.9873074\n",
            "Bags0.9999951\n",
            "Wallets0.9863436\n",
            "Shoes0.99966335\n",
            "Bottomwear0.99187005\n",
            "Bags0.99621564\n",
            "Watches0.9220701\n",
            "Bottomwear0.9959013\n",
            "Topwear0.87531257\n",
            "Shoes0.97002417\n",
            "Bags0.9997819\n",
            "Bags0.9901237\n",
            "Watches0.76581997\n",
            "Bottomwear0.9932834\n",
            "Bags0.9999995\n",
            "Shoes0.95127743\n",
            "Bags0.99993134\n",
            "Shoes0.85675865\n",
            "Watches0.98883635\n",
            "Watches0.7840736\n",
            "Shoes0.9991992\n",
            "Bags0.9999999\n",
            "Bags0.99909043\n",
            "Bags0.99999726\n",
            "Bottomwear0.9982911\n",
            "Wallets0.78665286\n",
            "Watches0.9095854\n",
            "Shoes0.9922416\n",
            "Bags0.99992406\n",
            "Bags0.9999075\n",
            "Lips0.68980104\n",
            "Shoes0.9999809\n",
            "Bottomwear0.9998042\n",
            "Bottomwear0.99999166\n",
            "Bags0.9921491\n",
            "Bags0.99999523\n",
            "Watches0.9993857\n",
            "Shoes0.90852654\n",
            "Bags0.9989728\n",
            "Shoes0.9530101\n",
            "Topwear0.9810765\n",
            "Bags0.9999987\n",
            "Shoes0.5525123\n",
            "Bottomwear0.99840575\n",
            "Bags0.9999467\n",
            "Bags0.99998665\n",
            "Bottomwear0.98791224\n",
            "Bags0.99790907\n",
            "Bottomwear0.99757785\n",
            "Bottomwear0.99934715\n",
            "Bottomwear1.0\n",
            "Shoes0.97737294\n",
            "Bags0.9992828\n",
            "Bags0.9768396\n",
            "Topwear0.7138744\n",
            "Bags0.96110237\n",
            "Topwear0.97416365\n",
            "Topwear0.4868519\n",
            "Topwear0.86582065\n",
            "Bags0.99907184\n",
            "Watches0.9939932\n",
            "Shoes0.9986713\n",
            "Bags0.9998685\n",
            "Shoes0.9779416\n",
            "Bottomwear0.9996075\n",
            "Bottomwear0.9871796\n",
            "Bottomwear0.9999852\n",
            "Bags0.9999598\n",
            "Bags0.99998724\n",
            "Topwear0.99613523\n",
            "Watches0.9962651\n",
            "Bags0.9999963\n",
            "Bottomwear1.0\n",
            "Topwear0.9964055\n",
            "Bags0.9536853\n",
            "Bags0.9830684\n",
            "Bottomwear0.9999999\n",
            "Watches0.9999999\n",
            "Bottomwear0.9997253\n",
            "Topwear0.54468554\n",
            "Shoes0.9995629\n",
            "Bags0.9999999\n",
            "Watches1.0\n",
            "Bags0.9998977\n",
            "Bags0.99998903\n",
            "Bottomwear0.9993912\n",
            "Bags0.9999994\n",
            "Shoes0.99612683\n",
            "Bags0.9997185\n",
            "Bags0.99999976\n",
            "Bags0.99999106\n",
            "Bags0.9992562\n",
            "Bags0.99948967\n",
            "Bags0.9599294\n",
            "Bags0.9999995\n",
            "Bottomwear0.9999579\n",
            "Bags0.9999287\n",
            "Bags0.99999297\n",
            "Bags1.0\n",
            "Bags0.9999962\n",
            "Bottomwear0.9973769\n",
            "Watches0.9995484\n",
            "Bottomwear0.9890548\n",
            "Shoes0.99221355\n",
            "Shoes0.9999181\n",
            "Bags0.98816466\n",
            "Bottomwear0.99991477\n",
            "Bottomwear0.9999938\n",
            "Watches0.99964654\n",
            "Watches0.9998547\n",
            "Bags0.9999684\n",
            "Bags0.9970391\n",
            "Watches0.9999994\n",
            "Watches0.7577383\n",
            "Bottomwear0.99308974\n",
            "Bags0.9996593\n",
            "Bags0.99690694\n",
            "Watches0.86675906\n",
            "Bags0.999969\n",
            "Bottomwear0.99995136\n",
            "Topwear0.89184695\n",
            "Bags0.99957854\n",
            "Bottomwear0.94384366\n",
            "Watches0.99956924\n",
            "Bags0.9999838\n",
            "Bags0.93905824\n",
            "Bags0.9998097\n",
            "Bottomwear0.99997663\n",
            "Bags0.9966807\n",
            "Bags0.9999349\n",
            "Watches0.9487938\n",
            "Bottomwear0.9999914\n",
            "Topwear0.9999045\n",
            "Bottomwear0.99980956\n",
            "Bags0.9929709\n",
            "Bottomwear0.95732814\n",
            "Shoes0.8863575\n",
            "Topwear0.96757674\n",
            "Shoes0.9715395\n",
            "Bottomwear1.0\n",
            "Shoes0.74417025\n",
            "Bottomwear0.999663\n",
            "Bottomwear0.98806775\n",
            "Bottomwear0.9999889\n",
            "Watches0.99968064\n",
            "Bags0.9974444\n",
            "Bottomwear0.99999666\n",
            "Bags0.9399705\n",
            "Bags0.9998652\n",
            "Watches0.99999523\n",
            "Bags0.9962901\n",
            "Bags0.9990108\n",
            "Bags0.99972445\n",
            "Bottomwear0.93635243\n",
            "Bags0.9645596\n",
            "Bags0.98082834\n",
            "Watches0.9994018\n",
            "Shoes0.9460573\n",
            "Shoes0.99145085\n",
            "Bags0.9999883\n",
            "Bags0.9999534\n",
            "Bags0.99318177\n",
            "Bottomwear0.89148515\n",
            "Bottomwear0.8718651\n",
            "Bottomwear0.99996936\n",
            "Bottomwear0.99834466\n",
            "Bottomwear0.99981767\n",
            "Bottomwear0.9525138\n",
            "Shoes0.5260437\n",
            "Bags0.99992096\n",
            "Watches0.9792235\n",
            "Bags0.99817145\n",
            "Bottomwear0.9730813\n",
            "Watches0.99742967\n",
            "Bottomwear0.9998111\n",
            "Bags0.9994306\n",
            "Bags0.9998598\n",
            "Bags0.99999464\n",
            "Bags0.9999325\n",
            "Wallets0.70254123\n",
            "Shoes0.86786574\n",
            "Bags0.99351645\n",
            "Topwear0.99888974\n",
            "Bags0.9999999\n",
            "Bags0.9808349\n",
            "Bags0.9719202\n",
            "Watches0.9972072\n",
            "Bags0.9992361\n",
            "Bags0.99195474\n",
            "Topwear0.9773501\n",
            "Bags0.98364985\n",
            "Bags0.9999604\n",
            "Bags0.99989426\n",
            "Bags0.99999905\n",
            "Bottomwear0.9999931\n",
            "Bottomwear0.9994199\n",
            "Shoes0.9999695\n",
            "Bottomwear1.0\n",
            "Bottomwear0.86654687\n",
            "Bottomwear0.999943\n",
            "Bottomwear0.99999774\n",
            "Bags0.99946517\n",
            "Shoes0.9610824\n",
            "Bags0.9998909\n",
            "Bottomwear0.9958716\n",
            "Bottomwear0.99999905\n",
            "Watches0.99999595\n",
            "Bags0.99833065\n",
            "Shoes0.9970643\n",
            "Bags0.99949753\n",
            "Bags0.99865156\n",
            "Shoes0.99736625\n",
            "Watches0.9980501\n",
            "Bags0.9999069\n",
            "Bags0.9993741\n",
            "Watches0.99999654\n",
            "Watches0.9855159\n",
            "Bottomwear0.99569607\n",
            "Bags0.99999654\n",
            "Bottomwear0.99956554\n",
            "Bags0.2967551\n",
            "Bottomwear0.85697055\n",
            "Topwear0.3658823\n",
            "Shoes0.9300579\n",
            "Bags0.9994641\n",
            "Bags0.9973404\n",
            "Watches0.86888915\n",
            "Bottomwear0.997154\n",
            "Watches0.9989299\n",
            "Watches0.9999995\n",
            "Bags0.9999926\n",
            "Bottomwear0.99999\n",
            "Bags0.98438185\n",
            "Shoes0.9748971\n",
            "Bags0.9999809\n",
            "Bottomwear0.85437673\n",
            "Bottomwear0.8807608\n",
            "Bags0.9999324\n",
            "Bottomwear0.98951614\n",
            "Bottomwear0.9603361\n",
            "Bags0.76259923\n",
            "Bags0.9229095\n",
            "Bottomwear0.98792356\n",
            "Watches0.93914413\n",
            "Watches0.9999598\n",
            "Bags1.0\n",
            "Bottomwear0.99999917\n",
            "Bottomwear0.9962717\n",
            "Bags0.9976816\n",
            "Bags0.99917275\n",
            "Bags0.9263521\n",
            "Bags0.9999821\n",
            "Bottomwear0.8495382\n",
            "Bottomwear0.9982838\n",
            "Bottomwear0.9999645\n",
            "Bags0.99998176\n",
            "Bottomwear0.99008113\n",
            "Bottomwear1.0\n",
            "Shoes0.9329759\n",
            "Bags0.99988675\n",
            "Bags0.9997149\n",
            "Bags0.91042066\n",
            "Bags0.9996531\n",
            "Bottomwear0.999984\n",
            "Bottomwear0.9999925\n",
            "Bottomwear0.999962\n",
            "Bags0.94589376\n",
            "Bags0.9987858\n",
            "Watches0.47511405\n",
            "Topwear0.99581844\n",
            "Bags0.9992071\n",
            "Shoes0.99711704\n",
            "Shoes0.9172159\n",
            "Watches0.9995208\n",
            "Bottomwear0.99881124\n",
            "Bottomwear0.9999262\n",
            "Shoes0.9344022\n",
            "Bags0.9999474\n",
            "Topwear0.99993634\n",
            "Bottomwear0.8484396\n",
            "Bags0.99852633\n",
            "Wallets0.91249174\n",
            "Bags0.99986506\n",
            "Bottomwear0.9999224\n",
            "Shoes0.99792707\n",
            "Watches0.9997609\n",
            "Shoes0.3534523\n",
            "Bottomwear0.9986217\n",
            "Watches0.99598455\n",
            "Shoes0.84739095\n",
            "Topwear0.90798646\n",
            "Shoes0.99978286\n",
            "Bottomwear0.99998105\n",
            "Bags0.9999474\n",
            "Watches0.9999999\n",
            "Bags0.99996054\n",
            "Wallets0.35679692\n",
            "Bags0.99947304\n",
            "Bottomwear0.999938\n",
            "Bags0.9964269\n",
            "Bags0.999882\n",
            "Bags0.96020454\n",
            "Bottomwear0.9998653\n",
            "Bags0.9999927\n",
            "Bags0.99985945\n",
            "Watches0.9978871\n",
            "Bags0.9960437\n",
            "Bottomwear0.99948174\n",
            "Watches0.9999995\n",
            "Bags0.99971086\n",
            "Bottomwear0.9986958\n",
            "Bags0.9996512\n",
            "Shoes0.97547376\n",
            "Bottomwear0.9108147\n",
            "Shoes0.9959611\n",
            "Bottomwear0.9891852\n",
            "Topwear0.64987737\n",
            "Shoes0.899807\n",
            "Bags0.99996006\n",
            "Watches1.0\n",
            "Topwear0.60609245\n",
            "Topwear0.9913009\n",
            "Topwear0.95052797\n",
            "Bottomwear0.99619544\n",
            "Bags0.99996054\n",
            "Bags0.9997342\n",
            "Bags0.9987834\n",
            "Bags0.9975172\n",
            "Bottomwear0.999567\n",
            "Topwear0.9999474\n",
            "Shoes0.6749698\n",
            "Shoes0.9999974\n",
            "Bags0.9982667\n",
            "Bags0.9999056\n",
            "Bottomwear0.998168\n",
            "Bottomwear0.9701399\n",
            "Bags0.9907103\n",
            "Shoes0.59711105\n",
            "Topwear0.96009046\n",
            "Bottomwear0.9999294\n",
            "Bottomwear0.9994221\n",
            "Bags0.9994549\n",
            "Shoes0.99812025\n",
            "Bottomwear0.9999989\n",
            "Watches1.0\n",
            "Bottomwear0.9993507\n",
            "Topwear0.94917065\n",
            "Bottomwear0.9912411\n",
            "Bags0.999116\n",
            "Bottomwear0.5737763\n",
            "Bags0.99999905\n",
            "Bags0.9999943\n",
            "Watches0.9836775\n",
            "Shoes0.9410028\n",
            "Shoes0.9347013\n",
            "Bottomwear0.9874418\n",
            "Bags0.9999943\n",
            "Bottomwear0.9987042\n",
            "Bottomwear0.968037\n",
            "Shoes0.99980897\n",
            "Shoes0.9366184\n",
            "Topwear0.99999547\n",
            "Watches1.0\n",
            "Bottomwear0.9794057\n",
            "Watches0.9999999\n",
            "Bottomwear0.9533444\n",
            "Shoes0.98508865\n",
            "Bottomwear0.99998677\n",
            "Bottomwear0.99144554\n",
            "Bottomwear0.99965715\n",
            "Shoes0.95723855\n",
            "Bottomwear0.99983966\n",
            "Bags0.9998134\n",
            "Bags0.9757557\n",
            "Bags0.99372476\n",
            "Bags0.9974663\n",
            "Shoes0.71361846\n",
            "Watches0.5243611\n",
            "Bags0.9999981\n",
            "Shoes0.6289879\n",
            "Bags0.34856147\n",
            "Bags0.9997663\n",
            "Bottomwear0.9710423\n",
            "Watches0.99999297\n",
            "Shoes0.9987257\n",
            "Topwear0.9980709\n",
            "Bottomwear0.9999931\n",
            "Bottomwear0.972725\n",
            "Shoes0.7298584\n",
            "Topwear0.99113816\n",
            "Shoes0.9817716\n",
            "Bags0.99532205\n",
            "Bags0.9999939\n",
            "Shoes0.60993296\n",
            "Topwear0.9995944\n",
            "Watches0.99898535\n",
            "Shoes0.9997842\n",
            "Topwear0.9998839\n",
            "Shoes0.985905\n",
            "Topwear0.9967783\n",
            "Bags0.999236\n",
            "Bags0.99066204\n",
            "Bottomwear0.8632205\n",
            "Bags0.999853\n",
            "Shoes0.8618831\n",
            "Bottomwear0.99415886\n",
            "Bags0.9997676\n",
            "Bottomwear0.9962159\n",
            "Topwear0.9628026\n",
            "Bags0.99995637\n",
            "Bottomwear0.92134\n",
            "Bags0.9977831\n",
            "Bags0.9888058\n",
            "Shoes0.7298449\n",
            "Bags0.9989901\n",
            "Bottomwear0.9997527\n",
            "Shoes0.6694051\n",
            "Bottomwear0.98861504\n",
            "Bags0.99979573\n",
            "Bottomwear0.9999974\n",
            "Bottomwear0.50374335\n",
            "Bottomwear0.9955468\n",
            "Bottomwear0.9999994\n",
            "Bags0.99814284\n",
            "Bottomwear0.9528382\n",
            "Shoes0.9987519\n",
            "Bottomwear0.9962131\n",
            "Bags0.89018166\n",
            "Bags0.9999236\n",
            "Topwear0.9960227\n",
            "Bottomwear0.99999046\n",
            "Shoes0.8129032\n",
            "Bags0.9994481\n",
            "Bags0.9990134\n",
            "Bags0.9996455\n",
            "Bottomwear0.99980193\n",
            "Bags0.99986696\n",
            "Shoes0.9365186\n",
            "Bottomwear0.9990797\n",
            "Bottomwear0.9909705\n",
            "Shoes0.9823695\n",
            "Bags0.9989209\n",
            "Shoes0.8473697\n",
            "Shoes0.9948756\n",
            "Bottomwear0.99998236\n",
            "Bags0.98614734\n",
            "Bags0.9916999\n",
            "Bottomwear0.9999893\n",
            "Bottomwear0.99897873\n",
            "Bags0.99945587\n",
            "Shoes0.9792585\n",
            "Bottomwear0.997419\n",
            "Bags0.99969304\n",
            "Bottomwear0.9999393\n",
            "Watches0.95737207\n",
            "Bottomwear0.999613\n",
            "Watches0.62315506\n",
            "Bottomwear0.9998934\n",
            "Bottomwear0.99904996\n",
            "Watches0.9999684\n",
            "Bottomwear0.9993325\n",
            "Bags0.9999852\n",
            "Bottomwear0.9510031\n",
            "Bags0.99835026\n",
            "Bottomwear0.9509251\n",
            "Bags0.99943775\n",
            "Bags0.9998828\n",
            "Shoes0.75666755\n",
            "Bottomwear0.999869\n",
            "Bags0.9998853\n",
            "Topwear0.53122556\n",
            "Bottomwear0.8526675\n",
            "Shoes0.998781\n",
            "Topwear0.8673599\n",
            "Watches0.99979824\n",
            "Bags0.97729445\n",
            "Bottomwear0.999938\n",
            "Bags0.9998565\n",
            "Bags0.9999994\n",
            "Bottomwear0.9972485\n",
            "Watches0.9660621\n",
            "Bags0.97660136\n",
            "Shoes0.9999378\n",
            "Dress0.99963725\n",
            "Bottomwear1.0\n",
            "Topwear0.28612638\n",
            "Shoes0.9728928\n",
            "Bags0.9998914\n",
            "Watches0.98854023\n",
            "Watches0.9980258\n",
            "Bottomwear0.9896365\n",
            "Bags0.99983025\n",
            "Bags0.9999945\n",
            "Bottomwear0.98661464\n",
            "Shoes0.82732207\n",
            "Topwear0.97072965\n",
            "Bags0.8622036\n",
            "Bags0.99983346\n",
            "Shoes0.9190158\n",
            "Bags0.99990296\n",
            "Bottomwear0.99998546\n",
            "Bottomwear0.99999976\n",
            "Bottomwear0.9912514\n",
            "Bottomwear0.9966633\n",
            "Bags0.99490684\n",
            "Shoes0.99363106\n",
            "Bags0.99814355\n",
            "Watches0.9810875\n",
            "Bags0.9999646\n",
            "Bags0.9999435\n",
            "Bottomwear0.9879127\n",
            "Bottomwear0.97374946\n",
            "Bags0.99881786\n",
            "Bags0.99814975\n",
            "Bottomwear0.99999917\n",
            "Watches0.9998832\n",
            "Bags0.99962115\n",
            "Topwear0.9564255\n",
            "Bags0.99995196\n",
            "Topwear0.99494356\n",
            "Bags0.99997044\n",
            "Shoes0.98538023\n",
            "Watches0.80485743\n",
            "Bags0.99960047\n",
            "Shoes0.53612506\n",
            "Bags0.9990859\n",
            "Topwear0.58822924\n",
            "Bags0.9991049\n",
            "Shoes0.99964345\n",
            "Topwear0.977047\n",
            "Bottomwear0.99663615\n",
            "Bottomwear0.999218\n",
            "Topwear0.99862707\n",
            "Topwear0.8498576\n",
            "Bags0.8111588\n",
            "Bottomwear0.8817715\n",
            "Bags0.99959\n",
            "Wallets0.9511838\n",
            "Bottomwear0.9839908\n",
            "Bags0.9987607\n",
            "Bottomwear0.99995387\n",
            "Bags0.99998665\n",
            "Bottomwear0.96120155\n",
            "Bottomwear0.99950945\n",
            "Bottomwear0.99929214\n",
            "Bottomwear0.99897647\n",
            "Shoes0.817675\n",
            "Bottomwear0.9994541\n",
            "Bottomwear0.98444957\n",
            "Bottomwear0.998976\n",
            "Bottomwear0.9972683\n",
            "Topwear0.99831885\n",
            "Topwear0.99968004\n",
            "Shoes0.99851805\n",
            "Bottomwear0.70402753\n",
            "Bags0.99209213\n",
            "Bags0.99993265\n",
            "Shoes0.98056763\n",
            "Bags0.99992585\n",
            "Topwear0.99624\n",
            "Bags0.9999957\n",
            "Bags0.9999974\n",
            "Bottomwear0.85036874\n",
            "Shoes0.9732895\n",
            "Bags0.99996173\n",
            "Bags0.8839481\n",
            "Bottomwear0.79549056\n",
            "Bottomwear0.99549735\n",
            "Watches0.999997\n",
            "Bottomwear0.9999893\n",
            "Topwear0.69790256\n",
            "Topwear0.97781414\n",
            "Bags0.9956807\n",
            "Bags0.93354523\n",
            "Bags0.99028265\n",
            "Bags0.9999999\n",
            "Shoes0.56687415\n",
            "Bags0.9983485\n",
            "Bags0.9776093\n",
            "Bottomwear0.9881493\n",
            "Bottomwear1.0\n",
            "Bottomwear0.9997037\n",
            "Shoes0.78986573\n",
            "Topwear0.87614816\n",
            "Bags0.85888547\n",
            "Bags0.9970753\n",
            "Shoes0.9732038\n",
            "Topwear0.7457827\n",
            "Bottomwear0.5470773\n",
            "Shoes0.99671656\n",
            "Bags0.99990916\n",
            "Watches0.971221\n",
            "Bottomwear0.9996138\n",
            "Topwear0.7614512\n",
            "Bottomwear0.9959745\n",
            "Bottomwear0.9977059\n",
            "Topwear0.8932806\n",
            "Bottomwear0.998214\n",
            "Topwear0.9995906\n",
            "Bottomwear0.8253681\n",
            "Bags0.99999607\n",
            "Bags0.999589\n",
            "Bags0.9999989\n",
            "Bottomwear0.99999714\n",
            "Bottomwear0.9976042\n",
            "Shoes0.55699855\n",
            "Watches1.0\n",
            "Bottomwear0.99393505\n",
            "Watches0.8113434\n",
            "Bags0.9992281\n",
            "Bottomwear0.9997427\n",
            "Bags0.9993759\n",
            "Bags0.9999263\n",
            "Bottomwear0.3718729\n",
            "Shoes0.95411646\n",
            "Bags0.99995875\n",
            "Bags0.99899846\n",
            "Bags0.9999809\n",
            "Bags0.99687696\n",
            "Bags0.99990034\n",
            "Topwear0.94623125\n",
            "Topwear0.5658121\n",
            "Bags0.9977906\n",
            "Bags0.9986946\n",
            "Shoes0.79617244\n",
            "Watches0.9729706\n",
            "Bottomwear0.99978894\n",
            "Bottomwear0.9997503\n",
            "Bags0.9999956\n",
            "Dress0.98300296\n",
            "Watches0.9999995\n",
            "Bottomwear0.99526775\n",
            "Bottomwear1.0\n",
            "Watches0.9999993\n",
            "Bottomwear0.9999889\n",
            "Bags0.9999356\n",
            "Bags0.99999166\n",
            "Bags0.9996544\n",
            "Bags0.9982967\n",
            "Shoes0.79519206\n",
            "Bottomwear0.96679664\n",
            "Bags0.9993211\n",
            "Bottomwear0.9998153\n",
            "Bottomwear1.0\n",
            "Topwear0.9339358\n",
            "Bottomwear0.9992737\n",
            "Bags0.9989526\n",
            "Bags0.99852425\n",
            "Bottomwear0.9908502\n",
            "Bags0.3447433\n",
            "Topwear0.9883929\n",
            "Bags0.9995403\n",
            "Bottomwear0.5948301\n",
            "Watches1.0\n",
            "Bags0.99916697\n",
            "Bottomwear0.9969428\n",
            "Bottomwear0.9992287\n",
            "Bags0.9999893\n",
            "Bottomwear0.99325377\n",
            "Bottomwear0.99880314\n",
            "Bags0.9992238\n",
            "Bottomwear0.5085762\n",
            "Shoes0.98257315\n",
            "Bags0.9897105\n",
            "Bottomwear0.9849363\n",
            "Bags0.99778396\n",
            "Bags0.9995492\n",
            "Bottomwear0.99793005\n",
            "Bottomwear0.99930024\n",
            "Bottomwear0.99998593\n",
            "Bottomwear0.99648935\n",
            "Bottomwear0.99992895\n",
            "Bags0.998879\n",
            "Bottomwear0.94414634\n",
            "Bags0.99996424\n",
            "Bottomwear0.9998424\n",
            "Shoes0.9891644\n",
            "Bottomwear0.99997616\n",
            "Shoes0.9970372\n",
            "Bags0.9995473\n",
            "Bags0.99964213\n",
            "Bottomwear0.99311674\n",
            "Bags0.99997604\n",
            "Shoes0.999997\n",
            "Topwear0.84375376\n",
            "Bags0.99999905\n",
            "Bags0.99886715\n",
            "Bags0.99992406\n",
            "Topwear0.9999881\n",
            "Bags0.9996731\n",
            "Bottomwear0.97147524\n",
            "Bottomwear0.99996996\n",
            "Bags0.9999945\n",
            "Bags0.9946531\n",
            "Topwear0.9189577\n",
            "Bottomwear0.9994248\n",
            "Bags0.9999201\n",
            "Bottomwear0.9999906\n",
            "Shoes0.99972445\n",
            "Bags1.0\n",
            "Bottomwear0.99962854\n",
            "Bags0.99991345\n",
            "Topwear0.3786469\n",
            "Bags0.994034\n",
            "Bags0.9999976\n",
            "Watches0.99982184\n",
            "Bottomwear0.9985808\n",
            "Shoes0.5155798\n",
            "Bottomwear0.99999106\n",
            "Bags0.8823038\n",
            "Topwear0.93442583\n",
            "Bags0.999178\n",
            "Bottomwear0.5429228\n",
            "Shoes0.9803602\n",
            "Bottomwear0.99999917\n",
            "Bags0.9999901\n",
            "Bottomwear0.9827253\n",
            "Shoes0.99997735\n",
            "Bags0.99999714\n",
            "Bags0.9885765\n",
            "Bags0.99978334\n",
            "Bags0.9997726\n",
            "Bags0.99968207\n",
            "Bags0.99947447\n",
            "Shoes0.5336311\n",
            "Watches0.99999475\n",
            "Bottomwear0.99999404\n",
            "Bags0.99369013\n",
            "Bottomwear0.9993857\n",
            "Bottomwear1.0\n",
            "Shoes0.98114866\n",
            "Shoes0.9996123\n",
            "Bottomwear0.99990475\n",
            "Bottomwear0.98817843\n",
            "Bags0.99997294\n",
            "Bags0.96775997\n",
            "Bags0.99992144\n",
            "Topwear0.98407125\n",
            "Bottomwear0.9917516\n",
            "Topwear0.9731208\n",
            "Bottomwear0.9603782\n",
            "Bags0.9999999\n",
            "Bottomwear0.9997634\n",
            "Bottomwear0.9999728\n",
            "Bottomwear0.9204916\n",
            "Bags0.9999918\n",
            "Shoes0.9839917\n",
            "Bags0.98553586\n",
            "Bags0.9994585\n",
            "Bags0.9999919\n",
            "Bags0.9999337\n",
            "Watches1.0\n",
            "Bags0.9999492\n",
            "Bottomwear0.9840352\n",
            "Topwear0.92621815\n",
            "Bags0.9999312\n",
            "Bags0.99915385\n",
            "Bottomwear0.98874855\n",
            "Bags0.99859077\n",
            "Bottomwear0.99761283\n",
            "Bottomwear0.99605966\n",
            "Bags0.99700576\n",
            "Topwear0.9998374\n",
            "Bottomwear0.99925023\n",
            "Bottomwear0.9999957\n",
            "Shoes0.79441977\n",
            "Bottomwear0.9999999\n",
            "Watches0.9999995\n",
            "Topwear0.9998313\n",
            "Bottomwear0.9999473\n",
            "Bottomwear0.9999994\n",
            "Bags0.9999291\n",
            "Bags0.9984723\n",
            "Bottomwear0.97739136\n",
            "Bags0.9999918\n",
            "Bottomwear0.9997886\n",
            "Shoes0.8875831\n",
            "Bottomwear0.9963301\n",
            "Bags0.9997968\n",
            "Bottomwear0.9997261\n",
            "Bags0.99999666\n",
            "Bottomwear0.99998546\n",
            "Topwear0.97714055\n",
            "Bags0.9506243\n",
            "Shoes0.9981811\n",
            "Topwear0.6183875\n",
            "Topwear0.86228156\n",
            "Bottomwear0.9999989\n",
            "Bags0.8809177\n",
            "Shoes0.99999356\n",
            "Bags0.9998336\n",
            "Watches0.99993014\n",
            "Shoes0.9975426\n",
            "Bags0.99920326\n",
            "Topwear0.99938524\n",
            "Bottomwear0.9993747\n",
            "Bottomwear0.999966\n",
            "Bags0.9998447\n",
            "Watches0.99946064\n",
            "Bottomwear0.9981501\n",
            "Topwear0.9987134\n",
            "Bottomwear0.99526006\n",
            "Bags0.9981109\n",
            "Bags0.99785763\n",
            "Bags0.99984646\n",
            "Bottomwear0.99998856\n",
            "Bags0.9573854\n",
            "Bottomwear0.96734834\n",
            "Bags0.9999993\n",
            "Bags0.99172497\n",
            "Watches0.9999999\n",
            "Bags0.999902\n",
            "Watches0.93959856\n",
            "Bags0.9863919\n",
            "Bags0.99960285\n",
            "Bottomwear0.7060085\n",
            "Bags0.9735624\n",
            "Shoes0.9711927\n",
            "Bottomwear0.99999714\n",
            "Bottomwear0.7541912\n",
            "Topwear0.9947525\n",
            "Bottomwear0.9993063\n",
            "Bags0.99563795\n",
            "Bottomwear0.99999917\n",
            "Bags0.99828017\n",
            "Bottomwear1.0\n",
            "Bags0.9999807\n",
            "Watches0.99713045\n",
            "Bags0.9999206\n",
            "Topwear0.99219394\n",
            "Bags0.99939\n",
            "Bottomwear0.9999808\n",
            "Bottomwear0.9952813\n",
            "Watches0.99999976\n",
            "Topwear0.8962849\n",
            "Wallets0.7081686\n",
            "Watches0.9810447\n",
            "Bottomwear1.0\n",
            "Bags0.9667516\n",
            "Bags0.96816987\n",
            "Topwear0.8420466\n",
            "Watches1.0\n",
            "Bottomwear0.999969\n",
            "Topwear0.7803788\n",
            "Bags0.6403437\n",
            "Bags1.0\n",
            "Bags0.9989341\n",
            "Watches0.9901551\n",
            "Shoes0.7286861\n",
            "Bottomwear0.9999889\n",
            "Topwear0.9990538\n",
            "Topwear0.91356003\n",
            "Bottomwear0.9999862\n",
            "Bags0.99962735\n",
            "Watches0.99308926\n",
            "Shoes0.9915454\n",
            "Shoes0.89595\n",
            "Topwear0.99905664\n",
            "Bags0.99994874\n",
            "Watches0.8185468\n",
            "Bottomwear0.80342376\n",
            "Bags0.99966633\n",
            "Bags0.9999167\n",
            "Bags0.9963548\n",
            "Topwear0.98793167\n",
            "Shoes0.9999858\n",
            "Bottomwear1.0\n",
            "Bottomwear0.99789524\n",
            "Bags0.9997451\n",
            "Bottomwear0.99948275\n",
            "Bags0.9999999\n",
            "Wallets0.9700827\n",
            "Topwear0.9999548\n",
            "Shoes0.9904641\n",
            "Bottomwear0.9999883\n",
            "Bags0.95638853\n",
            "Shoes0.97041005\n",
            "Shoes0.99492633\n",
            "Watches0.98494613\n",
            "Bags0.99999964\n",
            "Shoes0.75624454\n",
            "Shoes0.9995921\n",
            "Bottomwear0.99920183\n",
            "Bags0.99800605\n",
            "Bags0.9998062\n",
            "Bags0.9998605\n",
            "Watches0.9989999\n",
            "Shoes0.76789135\n",
            "Bags0.9999987\n",
            "Bottomwear0.7057128\n",
            "Bottomwear0.9371762\n",
            "Bags0.9935614\n",
            "Bottomwear0.98640907\n",
            "Bags0.99998164\n",
            "Bags0.99333054\n",
            "Shoes0.9649074\n",
            "Bottomwear0.99999917\n",
            "Bags0.9996301\n",
            "Bottomwear0.9466877\n",
            "Bags0.9975908\n",
            "Bags0.99990463\n",
            "Shoes0.9827802\n",
            "Bags0.9999672\n",
            "Bags0.99999547\n",
            "Bags0.9985513\n",
            "Bottomwear0.9994097\n",
            "Topwear0.9966221\n",
            "Topwear0.99996984\n",
            "Bottomwear0.9985475\n",
            "Shoes0.9656524\n",
            "Lips0.54802155\n",
            "Bottomwear0.99886227\n",
            "Wallets0.72777677\n",
            "Topwear0.99986637\n",
            "Shoes0.84618276\n",
            "Bottomwear0.99993336\n",
            "Bags0.99922216\n",
            "Shoes0.4981974\n",
            "Wallets0.9903038\n",
            "Shoes0.99873227\n",
            "Bags0.9999802\n",
            "Bags0.99999046\n",
            "Watches1.0\n",
            "Bags0.999948\n",
            "Bags0.99983025\n",
            "Bags0.996232\n",
            "Shoes0.99974054\n",
            "Bottomwear0.9999472\n",
            "Bottomwear0.99613947\n",
            "Bags0.9998908\n",
            "Bags0.9981122\n",
            "Bottomwear0.9999889\n",
            "Bags0.9993876\n",
            "Bags0.99963224\n",
            "Bags0.87220514\n",
            "Watches0.9999832\n",
            "Bags0.9999043\n",
            "Bags0.7672171\n",
            "Bottomwear0.99997807\n",
            "Bottomwear0.9996443\n",
            "Shoes0.9956227\n",
            "Bags0.995413\n",
            "Bottomwear0.99960214\n",
            "Bottomwear0.9999994\n",
            "Bags0.99854577\n",
            "Bottomwear0.9989899\n",
            "Dress0.9350189\n",
            "Bags0.9994357\n",
            "Topwear0.81577134\n",
            "Bags0.97738725\n",
            "Bottomwear0.9935567\n",
            "Bottomwear0.9998166\n",
            "Topwear0.9951591\n",
            "Bottomwear0.9995254\n",
            "Bottomwear0.9902092\n",
            "Bottomwear0.99848264\n",
            "Bottomwear0.9999305\n",
            "Wallets0.9386244\n",
            "Bags0.8615929\n",
            "Bags0.9997508\n",
            "Bags0.9985922\n",
            "Bags0.9999801\n",
            "Topwear0.9997217\n",
            "Bags0.9998222\n",
            "Bags0.98906344\n",
            "Bags0.9929074\n",
            "Shoes0.9999819\n",
            "Bottomwear0.99993026\n",
            "Bags0.98858285\n",
            "Bags0.9999933\n",
            "Watches1.0\n",
            "Watches0.9840221\n",
            "Bags0.9999285\n",
            "Watches0.9567857\n",
            "Bags0.97677726\n",
            "Bags0.9999981\n",
            "Bags0.9999552\n",
            "Bags0.95533866\n",
            "Shoes0.99340326\n",
            "Topwear0.9874765\n",
            "Topwear0.98884356\n",
            "Watches0.9575301\n",
            "Topwear0.79472166\n",
            "Bags0.99998164\n",
            "Bottomwear1.0\n",
            "Watches0.9977774\n",
            "Shoes0.9995459\n",
            "Topwear0.9998505\n",
            "Topwear0.98970115\n",
            "Wallets0.69819397\n",
            "Bottomwear0.99998486\n",
            "Bottomwear0.53619194\n",
            "Watches0.99914825\n",
            "Bottomwear0.99999356\n",
            "Bags0.99992335\n",
            "Bags0.99903524\n",
            "Shoes0.9936186\n",
            "Bags0.99999\n",
            "Topwear0.6904235\n",
            "Bags0.99999297\n",
            "Bags0.7174398\n",
            "Lips0.80644435\n",
            "Watches0.99999976\n",
            "Watches0.99899536\n",
            "Bottomwear0.9999994\n",
            "Bottomwear0.9999981\n",
            "Bags0.7039831\n",
            "Bottomwear0.9997845\n",
            "Bottomwear0.9624833\n",
            "Bags0.97526336\n",
            "Bags0.99728405\n",
            "Shoes0.93740314\n",
            "Bottomwear0.99985564\n",
            "Bottomwear0.9623234\n",
            "Bottomwear0.99997413\n",
            "Bottomwear0.9999784\n",
            "Bottomwear0.99407154\n",
            "Topwear0.47398964\n",
            "Bottomwear0.9973463\n",
            "Bottomwear0.99988043\n",
            "Bottomwear0.98115015\n",
            "Bags0.98964804\n",
            "Bags0.99986136\n",
            "Bottomwear0.9991623\n",
            "Bags0.5874831\n",
            "Bottomwear0.5035763\n",
            "Topwear0.59881175\n",
            "Bags0.81498885\n",
            "Watches0.99988616\n",
            "Bottomwear0.99356633\n",
            "Bags0.99963295\n",
            "Bags0.99897885\n",
            "Bottomwear0.999899\n",
            "Bags0.8922739\n",
            "Topwear0.9996859\n",
            "Dress0.99431956\n",
            "Bags0.99980026\n",
            "Bags0.98714256\n",
            "Bottomwear0.92262673\n",
            "Bags0.99916124\n",
            "Bags0.9965538\n",
            "Bottomwear0.99902105\n",
            "Bottomwear0.79855514\n",
            "Bags0.9996551\n",
            "Bottomwear1.0\n",
            "Watches0.9877407\n",
            "Bags0.99978644\n",
            "Bottomwear0.9992943\n",
            "Bottomwear0.9999732\n",
            "Topwear0.9160673\n",
            "Bags0.8888069\n",
            "Bags0.9989033\n",
            "Lips0.8931382\n",
            "Bags0.9992372\n",
            "Bags0.99992657\n",
            "Shoes0.9967535\n",
            "Watches0.99973077\n",
            "Shoes0.96981007\n",
            "Shoes0.61635846\n",
            "Watches1.0\n",
            "Bottomwear0.99991024\n",
            "Bottomwear0.9886874\n",
            "Bags0.9968502\n",
            "Bottomwear0.99464005\n",
            "Bottomwear0.9972447\n",
            "Bottomwear0.99999833\n",
            "Bags0.99993217\n",
            "Bags0.99974865\n",
            "Bags0.9841038\n",
            "Watches0.9975916\n",
            "Wallets0.98273486\n",
            "Topwear0.9941659\n",
            "Watches0.9999999\n",
            "Bottomwear0.9990891\n",
            "Bottomwear0.98827726\n",
            "Topwear0.9363009\n",
            "Bags0.999694\n",
            "Topwear0.995195\n",
            "Bottomwear0.9975605\n",
            "Shoes0.9921313\n",
            "Bags0.93062437\n",
            "Bags0.99886024\n",
            "Topwear0.99398535\n",
            "Bags0.94494313\n",
            "Bags0.9999658\n",
            "Bottomwear0.99482346\n",
            "Bags0.9999075\n",
            "Watches0.98786795\n",
            "Bottomwear0.93707496\n",
            "Bags0.51056415\n",
            "Shoes0.9986713\n",
            "Bags0.99987364\n",
            "Bottomwear0.8825598\n",
            "Bags0.98842156\n",
            "Topwear0.99619734\n",
            "Shoes0.5513308\n",
            "Bottomwear0.9954152\n",
            "Topwear0.99989915\n",
            "Topwear0.929635\n",
            "Wallets0.9385301\n",
            "Bottomwear0.97417736\n",
            "Bags0.99988675\n",
            "Bags0.9860599\n",
            "Bags0.99851817\n",
            "Bags0.9735097\n",
            "Bags0.9927658\n",
            "Bags0.9798891\n",
            "Bags0.99999475\n",
            "Watches0.45837232\n",
            "Bottomwear0.9994844\n",
            "Shoes0.6222267\n",
            "Bottomwear0.96421367\n",
            "Bags0.99287707\n",
            "Bags0.9985728\n",
            "Watches0.9909277\n",
            "Bags0.99999976\n",
            "Watches0.9987878\n",
            "Bottomwear0.97902465\n",
            "Shoes0.99577576\n",
            "Bottomwear0.731786\n",
            "Shoes0.8842866\n",
            "Watches0.9999999\n",
            "Shoes0.9900607\n",
            "Shoes0.9996716\n",
            "Shoes0.9972453\n",
            "Bottomwear0.99989927\n",
            "Bottomwear1.0\n",
            "Bags0.9973253\n",
            "Bags0.99028265\n",
            "Bags0.99098563\n",
            "Shoes0.84618276\n",
            "Watches1.0\n",
            "Bags0.99974054\n",
            "Bottomwear0.9964371\n",
            "Bottomwear0.9998073\n",
            "Watches0.9435584\n",
            "Bottomwear0.9997534\n",
            "Bags0.9996886\n",
            "Bottomwear0.67236555\n",
            "Bags0.9999372\n",
            "Bottomwear0.99999976\n",
            "Watches0.9994287\n",
            "Bags0.99618995\n",
            "Bags0.9946101\n",
            "Bottomwear0.98023087\n",
            "Shoes0.9864927\n",
            "Topwear0.9954724\n",
            "Bags0.99998057\n",
            "Shoes0.99744165\n",
            "Bags0.9999968\n",
            "Bags0.9999038\n",
            "Shoes0.99470997\n",
            "Bags0.9999927\n",
            "Shoes0.9279922\n",
            "Bottomwear0.5135317\n",
            "Bags0.9986998\n",
            "Shoes0.9989766\n",
            "Bottomwear0.99987936\n",
            "Bags0.9998128\n",
            "Bottomwear0.9995079\n",
            "Bottomwear0.9773136\n",
            "Watches0.698513\n",
            "Watches0.9387185\n",
            "Topwear0.76867115\n",
            "Bags0.99941814\n",
            "Bags0.999966\n",
            "Watches0.9999739\n",
            "Bottomwear0.997964\n",
            "Bottomwear0.9999999\n",
            "Bottomwear0.5831037\n",
            "Topwear0.61894864\n",
            "Dress0.9997769\n",
            "Shoes0.94214106\n",
            "Bottomwear0.7518333\n",
            "Bags0.99999976\n",
            "Bags0.99533457\n",
            "Bottomwear0.99898535\n",
            "Bags0.99370486\n",
            "Bags0.9999263\n",
            "Bags0.9999999\n",
            "Shoes0.7899334\n",
            "Bottomwear0.9999567\n",
            "Bottomwear0.99983585\n",
            "Bags0.9994475\n",
            "Bottomwear1.0\n",
            "Wallets0.8778359\n",
            "Topwear0.97345746\n",
            "Shoes0.983836\n",
            "Bottomwear0.92558444\n",
            "Bottomwear0.99965775\n",
            "Shoes0.7104726\n",
            "Bags0.999671\n",
            "Topwear0.99999547\n",
            "Dress0.99199206\n",
            "Topwear0.99939775\n",
            "Bottomwear0.6410915\n",
            "Bags0.9782186\n",
            "Bags0.99236745\n",
            "Bottomwear0.9981041\n",
            "Bottomwear0.9999174\n",
            "Bags0.88444203\n",
            "Bags0.9982322\n",
            "Watches0.88316166\n",
            "Bags0.99211085\n",
            "Bags0.9999125\n",
            "Bags0.99989843\n",
            "Shoes0.999092\n",
            "Bottomwear0.99908495\n",
            "Bags0.9999963\n",
            "Bags0.99476624\n",
            "Bags0.99956924\n",
            "Topwear0.9967476\n",
            "Bottomwear0.99949074\n",
            "Bottomwear0.9905235\n",
            "Bags0.9877885\n",
            "Bags0.99998605\n",
            "Shoes0.44515705\n",
            "Bottomwear0.99657744\n",
            "Lips0.5407949\n",
            "Bottomwear0.8377309\n",
            "Bags0.99977475\n",
            "Watches0.9632553\n",
            "Bottomwear0.9999789\n",
            "Bags1.0\n",
            "Bags0.79618037\n",
            "Bottomwear0.9302875\n",
            "Shoes0.90573484\n",
            "Dress0.9970687\n",
            "Bags0.9999839\n",
            "Bags0.99788386\n",
            "Bags0.99995005\n",
            "Bottomwear0.99772674\n",
            "Bags0.97954214\n",
            "Shoes0.97100776\n",
            "Shoes0.91286695\n",
            "Bags0.99667954\n",
            "Bags0.55825406\n",
            "Topwear0.7551862\n",
            "Bags0.9999522\n",
            "Bags0.9989003\n",
            "Shoes0.99820304\n",
            "Bottomwear0.99997103\n",
            "Bottomwear0.99992895\n",
            "Watches1.0\n",
            "Bottomwear0.99956256\n",
            "Shoes0.9961553\n",
            "Watches0.9385341\n",
            "Bags0.99355936\n",
            "Bottomwear0.99930274\n",
            "Bottomwear0.99981564\n",
            "Bags0.8260598\n",
            "Bags0.98331106\n",
            "Topwear0.79667866\n",
            "Bags0.9999943\n",
            "Bags0.9997929\n",
            "Shoes0.8024232\n",
            "Bags0.99999905\n",
            "Bottomwear0.99525577\n",
            "Bags0.9985128\n",
            "Topwear0.9837753\n",
            "Topwear0.5475733\n",
            "Bottomwear1.0\n",
            "Topwear0.7703345\n",
            "Bags0.99968326\n",
            "Bottomwear0.99999785\n",
            "Shoes0.75033176\n",
            "Topwear0.99998844\n",
            "Bottomwear0.99980575\n",
            "Bottomwear1.0\n",
            "Bags0.99998987\n",
            "Bags0.99998724\n",
            "Bags0.9996126\n",
            "Watches0.9848719\n",
            "Bags0.9993455\n",
            "Bags0.9993499\n",
            "Bottomwear0.9999901\n",
            "Bags0.999998\n",
            "Topwear0.8567426\n",
            "Watches0.9688158\n",
            "Bags0.99976295\n",
            "Bags0.99982977\n",
            "Bottomwear0.99999464\n",
            "Shoes0.82124925\n",
            "Bags0.9978435\n",
            "Bottomwear0.99985945\n",
            "Bags0.999997\n",
            "Bags0.9978916\n",
            "Wallets0.99752\n",
            "Shoes0.9138412\n",
            "Topwear0.9937587\n",
            "Bags0.9993506\n",
            "Bags0.99958473\n",
            "Bags0.9923104\n",
            "Bottomwear0.9994992\n",
            "Shoes0.9991386\n",
            "Shoes0.9999008\n",
            "Bottomwear0.98795974\n",
            "Bags0.99476755\n",
            "Watches0.9999993\n",
            "Bags0.99092895\n",
            "Bags0.9998288\n",
            "Bottomwear0.99999905\n",
            "Bags0.9999006\n",
            "Bags0.999998\n",
            "Bags0.99969494\n",
            "Bags0.99991703\n",
            "Topwear0.99998784\n",
            "Bags0.99983835\n",
            "Bags0.99806696\n",
            "Bottomwear0.9968027\n",
            "Bags0.9999808\n",
            "Bags0.9999994\n",
            "Bags0.9917836\n",
            "Topwear0.99992037\n",
            "Bags0.99975246\n",
            "Topwear0.9993437\n",
            "Topwear0.9999151\n",
            "Bottomwear0.99000543\n",
            "Bags0.9999999\n",
            "Bags0.9999604\n",
            "Bags0.99995875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpSQ3rhB_KJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlGSh0byGW5z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b6f9a206-cfc7-4a67-b2fc-904f02dba819"
      },
      "source": [
        "submission = pd.DataFrame({ 'imageId': df[3500:].image.values, 'label': allpredictedlabels })\n",
        "submission.to_csv(\"my_submission.csv\", index=False)\n",
        "submission"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageId</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30857.jpg</td>\n",
              "      <td>Watches0.9943059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19584.jpg</td>\n",
              "      <td>Bottomwear0.9997248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33825.jpg</td>\n",
              "      <td>Bags0.99986434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40545.jpg</td>\n",
              "      <td>Watches0.9998704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29522.jpg</td>\n",
              "      <td>Bottomwear0.9924642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>48343.jpg</td>\n",
              "      <td>Topwear0.9999151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>48917.jpg</td>\n",
              "      <td>Bottomwear0.99000543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>8111.jpg</td>\n",
              "      <td>Bags0.9999999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>44379.jpg</td>\n",
              "      <td>Bags0.9999604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>25983.jpg</td>\n",
              "      <td>Bags0.99995875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1500 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        imageId                 label\n",
              "0     30857.jpg      Watches0.9943059\n",
              "1     19584.jpg   Bottomwear0.9997248\n",
              "2     33825.jpg        Bags0.99986434\n",
              "3     40545.jpg      Watches0.9998704\n",
              "4     29522.jpg   Bottomwear0.9924642\n",
              "...         ...                   ...\n",
              "1495  48343.jpg      Topwear0.9999151\n",
              "1496  48917.jpg  Bottomwear0.99000543\n",
              "1497   8111.jpg         Bags0.9999999\n",
              "1498  44379.jpg         Bags0.9999604\n",
              "1499  25983.jpg        Bags0.99995875\n",
              "\n",
              "[1500 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EejTPdTK2RGn",
        "colab_type": "text"
      },
      "source": [
        "# conclusion / The results:\n",
        "\n",
        "* I have used 6 models to see which performs best on the given dataset\n",
        "* The Vgg19 Performed very well on the given dataset with accuracy(91%) and validation_acc(88%)\n",
        "* Therefore , I have used Its weights(Vgg19_model_weights) to predict the label of images\n",
        "\n",
        "#  Thanks  and suggestions for  improvements are always  welcome\n",
        "#  satyamsharma."
      ]
    }
  ]
}